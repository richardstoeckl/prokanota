# Snakemake workflow: `Prokanota`

[![Snakemake](https://img.shields.io/badge/snakemake-≥9.3.5-brightgreen.svg)](https://snakemake.github.io)

Flexible [Snakemake](https://snakemake.github.io) pipeline for **proka**ryotic **an**n**ota**tion with a code-free* ,modular database architecture.

## About

Prokaryotic genome annotation is a complex, multi-faceted task. Excellent tools like [Prokka](https://github.com/tseemann/prokka) and [Bakta](https://github.com/oschwengers/bakta) provide streamlined, user-friendly annotation that works well for most bacterial genomes. However, their hierarchical annotation systems can lose important nuances, and they often struggle with non-standard organisms—particularly archaeal species—where database coverage is more limited.

On the other end of the spectrum, sophisticated custom annotation pipelines like those described by [Dombrowski, Nina, et al. Nature Communications, vol. 11, no. 1, Aug. 2020, p. 3939](https://doi.org/10.1038/s41467-020-17408-w) provide exceptional flexibility and depth, but require substantial bioinformatics expertise and manual configuration.

**Prokanota fills the gap between these approaches.** It provides:

- **An efficient, well-tested backbone** that handles gene prediction and feature annotation automatically
- **Detailed documentation** (in the wiki) and helper scripts to facilitate database setup
- **A modular, config-driven database system** that allows users to customize their annotations without writing code
- **Multi-database top-hit reporting** that preserves annotation nuances and allows informed manual curation

This design philosophy means you can start with the well-documented databases ([CDD](https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd_help.shtml#NCBI_curated_domains), [COG](https://www.ncbi.nlm.nih.gov/COG/), [arCOG](https://pubmed.ncbi.nlm.nih.gov/25764277/), [PGAP](https://ftp.ncbi.nlm.nih.gov/hmm/); see the the [wiki](https://github.com/richardstoeckl/prokanota/wiki)) and gradually add custom HMM or RPS-BLAST databases as your annotation needs evolve—no Snakemake expertise required.

## Features

### Gene Prediction
1. **Deterministic gene IDs:** Gene IDs are generated by hashing the DNA content and sample ID, ensuring consistency across runs and making changes immediately identifiable. See FAQ for details.
2. **CDS prediction** using [pyrodigal](https://github.com/althonos/pyrodigal), which fixes edge-case bugs from the original Prodigal.
3. **rRNA prediction** using [pybarrnap](https://github.com/moshi4/pybarrnap) in `--accurate` mode with Rfam(14.10) covariance models.
4. **tRNA prediction** using [tRNAscan-SE](https://github.com/UCSC-LoweLab/tRNAscan-SE) in `-G` mode for general tRNA prediction.

### Annotation System
5. **Modular database architecture:** Add custom HMM or RPS-BLAST databases by editing a config file—no code changes needed.
6. **Multi-database annotation:** Top hits from each database are reported separately, preserving annotation depth and enabling informed manual curation.

### Output Formats
7. **Pangenome-ready outputs:** Annotations are provided in `.gff`, `.tsv`, and `.gbk` formats, with sequences in `.fna` (nucleotide) and `.faa` (protein) formats. All use consistent gene IDs for easy cross-referencing with downstream tools.

## Modular Database System

The pipeline uses a **config-driven modular architecture** that separates database definitions from code. This means:

- **No code editing required:** Add new databases by editing `config/databases.yaml`
- **Flexible tool support:** Currently supports `pyhmmer` (for HMM databases) and `rpsblast` (for RPS-BLAST databases) with more to come!
- **Automatic rule generation:** The Snakemake workflow dynamically generates search and parse rules for each enabled database
- **Standardized output:** All databases contribute columns to a unified annotation table

## Usage

**[Check out the usage instructions in the snakemake workflow catalog](https://snakemake.github.io/snakemake-workflow-catalog/docs/workflows/richardstoeckl%20prokanota.html)**

But here is a rough overview:
1. Install [conda](https://docs.conda.io/en/latest/miniconda.html) (miniforge or miniconda is fine).
2. Install snakemake with:
```bash
conda install -c conda-forge -c bioconda snakemake
```
3. [Download the latest release from this repo](https://github.com/richardstoeckl/prokanota/releases/latest) and cd into it, or download the development version [directly from github](https://github.com/richardstoeckl/prokanota/archive/refs/heads/main.zip)
4. Edit the `config/config.yaml` to provide the paths to your results/logs directories, and the path to where you want the databases to be downloaded to.
5. Edit the `config/databases.yaml` to enable/disable databases or add custom ones.
6. Edit the `config/metadata.csv` file with the specific details for each assembly you want to annotate. Please note, that the sampleID you enter here will influence the naming of the contig and gene IDs!
7. Open a terminal in the main dir and start a dry-run of the pipeline with the following command. This will show you if you set up the paths correctly:

```bash
snakemake --sdm conda -n --cores
```
8. Run the pipeline with
```bash
snakemake --sdm conda --cores
```

### Adding a Custom Database (quick overview)

1. Prepare your database:
   - For HMM: Press your `.hmm` files using `hmmpress` or download ready-to-use db.
   - For RPS-BLAST: Build your database using `makeprofiledb` or download ready-to-use db.
2. Create a mapping file (TSV, no header): `accession<TAB>short_name<TAB>description<TAB>category`
3. Add an entry to `config/databases.yaml`:
   ```yaml
   databases:
     - name: MyCustomDB
       enabled: true
       order: 50 # lower = earlier
       search_tool: pyhmmer  # or rpsblast
       db_path: "/path/to/db.hmm"
       mapping_path: "/path/to/mapping.tsv"
       evalue_cutoff: 1.0e-3
       columns:
         - {name: MyDB_hit, source: query_name}
         - {name: MyDB_description, source: description}
         - {name: MyDB_evalue, source: evalue}
   ```
4. Run the pipeline—your custom database will be searched automatically

See the [wiki](https://github.com/richardstoeckl/prokanota/wiki) for detailed instructions and helper scripts.


## Recommended Databases to use
- **Databases**
    - **[CDD](https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd_help.shtml#NCBI_curated_domains) - The Conserved Domains Database curated by NCBI.** *Sources information from the [SMART](http://smart.embl-heidelberg.de/), [Pfam](http://pfam.sanger.ac.uk/), [COG](https://www.ncbi.nlm.nih.gov/COG/), [TIGR](https://www.ncbi.nlm.nih.gov/Structure/cdd/docs/tigrfams.html), and [PRK](https://www.ncbi.nlm.nih.gov/proteinclusters) databases. This makes it a good consolidated database for annotation purposes.*
    - **[COG](https://www.ncbi.nlm.nih.gov/COG/) - The Clusters of Orthologous Genes (COG) database.** *Even though the CDD sources some information from this DB as well, this is such a good resource that it deserves to be included in full and on its own.*
    - **[arCOG](https://pubmed.ncbi.nlm.nih.gov/25764277/) - The archaea specific version of the COG database.** *This is included to serve more up-to-date and accurate annotation of archaeal genomes.*
    - **[PGAP](https://ftp.ncbi.nlm.nih.gov/hmm/) - HMMs are used by the [NCBI Prokaryotic Genome Annotation Pipeline (PGAP)](https://pubmed.ncbi.nlm.nih.gov/33270901/).** *Since most submissions of novel prokaryotic genomes are evaluated by the PGAP tool, it makes sense to include its DB.*

## FAQ

- **How is the gene_id determined? What do the letters mean?**
  - The gene_id is determined by the contig_id, which is determined by hashing the DNA sequence of the genome and the user supplied sampleID. This ensures that the gene_id is consistent across runs. This is important for downstream analyses, as it allows you to cross-reference your results with the annotations from this pipeline. Changes in the contig_id indicate either changes in the sampleID or changes in the genome sequence and should probably investigated. The hashing algorithm is found in [`prokanota/workflow/scripts/features.py`](https://github.com/richardstoeckl/prokanota/blob/c2dc89227d3171187cf6949af15b27dddd4aa390/workflow/scripts/features.py#L564-L590).
- **How is the predicted protein molecular weight estimated?**
  - The molecular weight is estimated by summing the average residue masses in the protein sequence and adding the mass of one water molecule, as described in [GASTEIGER, Elisabeth, et al. The proteomics protocols handbook, 2005, S. 571-607](https://doi.org/10.1385/1-59259-890-0:571). The molecular weights are taken from the [here](https://web.expasy.org/findmod/findmod_masses.html#AA) and the function can be found in [`prokanota/workflow/scripts/features.py`](https://github.com/richardstoeckl/prokanota/blob/c2dc89227d3171187cf6949af15b27dddd4aa390/workflow/scripts/features.py#L398-L416)
- **Can I use this pipeline for bacterial genomes?**
  - Absolutely! While the pipeline was designed with archaeal annotation challenges in mind, it works equally well for bacterial genomes.
- **What if I don't want to use all the built-in databases?**
  - Simply set `enabled: false` for any database in `config/databases.yaml`.

```
Copyright Richard Stöckl 2025.
Distributed under the Boost Software License, Version 1.0.
(See accompanying file LICENSE or copy at 
https://www.boost.org/LICENSE_1_0.txt)
```